{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hangjoni/nlp/blob/main/Transformer_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Code Walkthrough\n",
        "\n",
        "[Mark Riedl](http://eilab.gatech.edu/mark-riedl.html)\n",
        "\n",
        "This notebook walks through a single forward pass of the Transformer architecture in pytorch. It is meant for illustration and educational purposes only.\n",
        "\n",
        "The Transformer was introduced by Vaswani et al. (2017) in their paper, titled [Attention Is All You Need](https://arxiv.org/abs/1706.03762)."
      ],
      "metadata": {
        "id": "CoRK-2VVfbMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computation Graph"
      ],
      "metadata": {
        "id": "9riNZp-Jf-HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the computation graph, an illustrated diagram of the mathematical operations, their inputs and their outputs. The inputs at the bottom are fed upward into an encode and a decoder (depicted side by side like in a sequence-to-sequence network). At every stage, it shows the matrix and their shapes (excluding the batching dimension, which makes the tensors more complicated looking without adding much information). The bubbles show what part of the code below is responsible for each part of the diagram.\n",
        "\n",
        "![Computation Graph](https://www.dropbox.com/s/bjwdq06zvq703b4/transformer.png?dl=1)"
      ],
      "metadata": {
        "id": "w4kYasNjgBKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ESzQqTWmfDJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "S-cmUU-75qcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameters"
      ],
      "metadata": {
        "id": "fl8-t60g85Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_embed = 512       # embedding size for the attention modules\n",
        "num_heads = 8       # Number of attention heads\n",
        "num_batches = 1     # number of batches (1 makes it easier to see what is going on)\n",
        "vocab = 50000       # vocab size\n",
        "max_len = 5000      # Max length of x and y?\n",
        "n_layers = 1        # number of attention layers (not used but would be an expected hyper-parameter)\n",
        "d_ff = 2048         # hidden state size in the feed forward layers\n",
        "epsilon = 1e-6      # epsilon to use when we need a small non-zero number\n"
      ],
      "metadata": {
        "id": "99lu6YH3kxwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make dummy data"
      ],
      "metadata": {
        "id": "xt8SMUhu5bSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create some dummy input data, consisting of three tokens. The 2nd token will be the masked token. Initially we have an input `x` of size `batch_size x sequence_length`. Throughout, we will use `x` to denote the tensor that originated from the input sequence, and `y` to denote the tensor that originated from the target sequence."
      ],
      "metadata": {
        "id": "KCzWWhuvfs9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1, 2, 3]]) # input will be 3 tokens\n",
        "y = torch.tensor([[1, 2, 3]]) # target will be same as the input for many applications\n",
        "x_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd input token\n",
        "y_mask = torch.tensor([[1, 0, 1]]) # Mask the 2nd target token\n",
        "print(\"x\", x.size())\n",
        "print(\"y\", y.size())"
      ],
      "metadata": {
        "id": "AVjmrgvysDd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Encoder"
      ],
      "metadata": {
        "id": "QG8oYLnl8_mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section shows a walk-through of one attention layer in the encoder. The purpose of the encoder is to create a *hidden state*, an encoded representation of the input sequence. The hidden state is then passed to the decoder."
      ],
      "metadata": {
        "id": "n4NB3UzKgUAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Encoder Embeddings"
      ],
      "metadata": {
        "id": "yvhxS-vm9QMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a conventional embedding layer to convert the tokens into embeddings of size `d_embed`. The embedding activations are then scaled by `sqrt(d_model)` in order to make them bigger. This will be important when positional embedding information is added (next). We want this embedding information to have more importance. The result is a tensor of size `batch_size x sequence_length x embedding_size`."
      ],
      "metadata": {
        "id": "KgtVhtKegehe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the embedding module. It understands that each token should result in a separate embedding.\n",
        "emb = nn.Embedding(vocab, d_embed)\n",
        "x = emb(x)\n",
        "# Scale the embedding\n",
        "x = x * math.sqrt(d_embed)\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "VKckLst4k4_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we add positional embedding information. The code below creates a pattern of overlapping sine and cosine waves that are added to the embedding. This differentiates embedded tokens based on where they are in the sequence. That is, if an input sequence has two of the same token, their embeddings will end up looking a little bit different based on their position in the sequence."
      ],
      "metadata": {
        "id": "CYsqRpXesZtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with an empty tensor\n",
        "pe = torch.zeros(max_len, d_embed, requires_grad=False)\n",
        "# array containing index values 0...max_len\n",
        "position = torch.arange(0, max_len).unsqueeze(1)\n",
        "divisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
        "# Make overlapping sine and cosine wave inside positional embedding tensor\n",
        "pe[:, 0::2] = torch.sin(position * divisor)\n",
        "pe[:, 1::2] = torch.cos(position * divisor)\n",
        "pe = pe.unsqueeze(0)\n",
        "# Add the position embedding to the main embedding\n",
        "x = x + pe[:, :x.size(1)]\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "WTaufnQksbjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how positional embeddings work, we can visualize the values that get added to each embedding in each dimension of the embedding (we only visualize the first 8 dimensions)."
      ],
      "metadata": {
        "id": "xTHKykBG4z6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))   # Make a plot\n",
        "d_embed_plot = 16             # for illustration purposes, set embedding dimensions = 16\n",
        "pe_plot = torch.zeros(max_len, d_embed_plot, requires_grad=False) # positional embedding tensor\n",
        "position_plot = torch.arange(0, max_len).unsqueeze(1)\n",
        "divisor_plot = torch.exp(torch.arange(0, d_embed_plot, 2) * -(math.log(10000.0) / d_embed_plot))\n",
        "pe_plot[:, 0::2] = torch.sin(position_plot * divisor_plot)\n",
        "pe_plot[:, 1::2] = torch.cos(position_plot * divisor_plot)\n",
        "pe_plot = pe_plot.unsqueeze(0)\n",
        "# plot it\n",
        "y_plot = torch.zeros(1, 50, d_embed_plot)\n",
        "y_plot = pe_plot[:, :y_plot.size(1)]\n",
        "plt.plot(np.arange(50), y_plot[0, :, 0:4].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in range(8)])"
      ],
      "metadata": {
        "id": "Ka5VtFT5iLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Encoder Attention Layers"
      ],
      "metadata": {
        "id": "0m4932dp9TOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sub-layers in this section will be repeated N times. This code walkthrough will only take us through one. The Encoder Attention Layers consist of a **self-attention** module followed by a **feed forward** module.\n",
        "\n",
        "The self-attention and the feed forward are wrapped with residuals. A residual connection adds the input of a block to the output of the block. Thus one can think of the block as trying to learn how to add or subtract from the input. This provides stability to the training because the block is not entirely responsible for everything that happens in the forward and backward passes. Taking a look at the encoder for the transformer, one can see the residual connections bypassing the self-attention providing a direct linkage to the hidden state. That is, the embedding at the bottom has the option of doing a lot of the heavy-lifting in terms of the final hidden state encoding. Self-attention and the other sub-layers may add a little bit to that final hidden state or a lot if it helps with loss. Another way of thinking about residuals is like sub-routines in conventional computer program that compute some side-effect. One sub-routine computes the final hidden state. Another sub-routine branches off and computes self-attention. But because every module must be on a gradient path, the side-routines must contribute something to the final loss."
      ],
      "metadata": {
        "id": "63Fk4B5U8Ht5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Self-Attention Module"
      ],
      "metadata": {
        "id": "RAwy0gSVAtAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.1 Set aside residual"
      ],
      "metadata": {
        "id": "HijbI99O8LNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A residual adds the inputs back into the outputs so that what happens in between can be thought of as computing a delta to the original.\n",
        "\n",
        "Typically we don't need to perform a `clone()` to create a residual, but we are using the same `x` variable in every step so the clone makes sure we don't overwrite."
      ],
      "metadata": {
        "id": "Wj4Tw3Ec9GGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_residual = x.clone()\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "n5MS2WtY46Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.2 Pre-Self-Attention Layer Normalization"
      ],
      "metadata": {
        "id": "XmMC5eFW7bF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we compute self-attention, we perform layer normalization. Layer normalization stabilizes the training by decreasing the chances that values start to go to extremes. This is accomplished by centering all the values relative to the mean."
      ],
      "metadata": {
        "id": "deaGpdDo0ezK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = x.mean(-1, keepdim=True)\n",
        "std = x.std(-1, keepdim=True)\n",
        "W1 = nn.Parameter(torch.ones(d_embed))\n",
        "b1 = nn.Parameter(torch.zeros(d_embed))\n",
        "x = W1 * (x - mean) / (std + epsilon) + b1\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "h0Bmhybj7cia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.3 Self-Attention"
      ],
      "metadata": {
        "id": "PhNCyBcLlNhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention is a process of generating scores that indicate how each token is to every other token. Thus we would expect a `seq_length x seq_length` matrix of values between 0 and 1, each indicating the importance of the i-th token to the j-th token. What does it mean to be \"relevant\"? Whatever reduces loss. The model must learn how to produce the scores.\n",
        "\n",
        "A metaphor for understanding self-attention is a hash table. In a hash table, there is a list of keys, each of which is associated with a value. A query is sent to the hash table, and the hash table has to find the matching key and return the associated value. Except imagine that this hash table is a fuzzy hash table in the sense that it the query doesn't have to match any keys and the hash table will return whatever seems closest to the query.\n",
        "\n",
        "The input to self-attention is a `batch_size x sequence_length x embedding_size` matrix. Ignoring the batching dimension, what we have is a sequence of embedded tokens. Self-attention copies this input, `x`, three times and calls them the \"query\" (`q`), \"keys\" (`k`), and \"values\" (`v`). Each of those matrices go through a linear layer. This linear layer is where the network learns to make scores. It makes each matrix different, and if it comes up with the right, different, matrices, it will get good attention scores. If it gets good attention scores and if it gets good attention scores, the loss will be reduced.\n",
        "\n",
        "Attention-scores are generated as follows. First, we split the `q` and `k` matrices into multiple parts (called \"heads\"). This is called multi-headed attention. The reason we do this is so that each head/part can independently produce different attention scores. This allows each token to have several \"best\" other tokens. In implementation, we just designate chunks of each token embedding to different heads.\n",
        "\n",
        "The `q` and `k` tensors are multiplied together. This creates a `batch_size x num_heads x sequence_length x sequence_length` matrix. Ignoring batching and heads, one can interpret this matrix as containing the raw scores where each cell computes how related the i-th token is to the j-th token (i is the row and j is the column).\n",
        "\n",
        "Next we pass this matrix through a softmax layer. The secret to softmax is that it can act like an argmax---it can pick the best match. Softmax squishes all values along a particular dimenion into 0...1. But what it is really doing is trying to force one particular cell to have a number close to 1 and all the rest close to 0. If we multiply this softmaxed score matrix to the `v` matrix, we are in essence asking (for each head), which column is best for each row. Recall that rows and columns correspond to tokens. So we are asking, which token goes best with every other token. Again, if the earlier linear layers get their parameters right, this multiplication will make good choices and loss will improve.\n",
        "\n",
        "At this point we can think of the softmaxed scores multiplied against `v` as tryinng to zero out everything but the most relevant token embedding (several because of multiple heads). The result, which we will store back in `x` for consistency is mainly the most-attended token embedding (several because of multiple heads) plus a little bit of every other embedded token sprinkled in because we can't do an actual argmax---the best we can do is get everything irrelevant to be close to zero so it doesn't impact anything else.\n",
        "\n",
        "This multiplication of the scores against the `v` matrix is what we refer to as *self-attention*. It is essentially a dot-product with an underlying learned scoring function. It basically tells us where we should look for good information. The Decoder will use this later."
      ],
      "metadata": {
        "id": "IiXzh1P3_7rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make three versions of x, for the query, key, and value\n",
        "# We don't need to clone because these will immediately go through linear layers, making new tensors\n",
        "k = x # key\n",
        "q = x # query\n",
        "v = x # value\n",
        "# Make three linear layers\n",
        "# This is where the network learns to make scores\n",
        "linear_k = nn.Linear(d_embed, d_embed)\n",
        "linear_q = nn.Linear(d_embed, d_embed)\n",
        "linear_v = nn.Linear(d_embed, d_embed)\n",
        "# We are going to fold the embedding dimensions and treat each fold as an attention head\n",
        "d_k = d_embed // num_heads\n",
        "# Pass q, k, v through their linear layers\n",
        "q = linear_q(q)\n",
        "k = linear_k(k)\n",
        "v = linear_v(v)\n",
        "# Do the fold, treating each h dimenssions as a head\n",
        "# Put the head in the second position\n",
        "q = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "k = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "v = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "print(\"q\", q.size())\n",
        "print(\"x\", k.size())\n",
        "print(\"v\", v.size())"
      ],
      "metadata": {
        "id": "kpExj3wl5uRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To produce the attention scores we multiply `q` and `k` (and normalize). We need to apply the mask so masked tokens don't attend to themselves. Apply softmax to emulate argmax (good stuff close to 1 irrelevant stuff close to 0). You won't see this happen if you look at `attn` because the linear layers aren't trained yet. The attention scores are finally applied to `v`."
      ],
      "metadata": {
        "id": "dkJ2lxguABuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.size(-1)\n",
        "# Compute the raw scores by multiplying k and q (and normalize)\n",
        "scores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "# Mask out the scores\n",
        "scores = scores.masked_fill(x_mask == 0, -epsilon)\n",
        "# Softmax the scores, ideally creating one score close to 1 and the rest close to 0\n",
        "# (Note: this won't happen if you look at the numbers because the linear layers haven't\n",
        "# learned anything yet.)\n",
        "attn = F.softmax(scores, dim = -1)\n",
        "print(\"attention\", attn.size())\n",
        "# Apply the scores to v\n",
        "x = torch.matmul(attn, v)\n",
        "print(\"x\", x.size())"
      ],
      "metadata": {
        "id": "CsYkRUoIDDfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is an illustration of what self-attention is doing. In the `attn` matrix below each row and column represents a different positionn in the input sequence, such that `attn[i][j]` is how much affinity the i-th position has for the j-th position. In a perfect world, the softmax pushes one element in each row close to 1 and everything else close to 0. Multiplying `attn` against `v` we are picking an embedding (hidden state) for each position (if we have multi-headed attention then we are picking several and adding combining them but the cell below doesn't show that)."
      ],
      "metadata": {
        "id": "K2tJuasVgbzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make fake attention scores with extreme values\n",
        "attn = torch.zeros(3, 3)\n",
        "attn[0,1] = 1\n",
        "attn[1,2] = 1\n",
        "attn[2,0] = 1\n",
        "print(\"attn:\")\n",
        "print(attn)\n",
        "# Make a fake v embedding\n",
        "v = torch.tensor(list(map(lambda x:list(range(x*10,(x*10)+10)), list(range(3))))).float()\n",
        "print(\"v:\")\n",
        "print(v)\n",
        "print(\"Matmul result:\")\n",
        "print(torch.matmul(attn, v))"
      ],
      "metadata": {
        "id": "z0W5rFyfeSWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But now your embeddings are all swapped around and a little bit of many positions can be mixed together. This is why the residual is going to be important because you can't lose the original embeddings in their original positions."
      ],
      "metadata": {
        "id": "mGAk--KSiSED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recombine the multiple attention heads (unfold)."
      ],
      "metadata": {
        "id": "6TFWgbA8lRl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.transpose(1, 2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "mdlAJY9UJeUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.4 Post-Self-Attention Feed forward"
      ],
      "metadata": {
        "id": "aLze8Ea5Beiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this point, we have some token embeddings pushed toward 1 and some token embeddings pushed toward 0. We need to prepare this matrix to be added back into the residual. That is, whatever comes out of this transformation has be a set of values that change the original embedding values for each token by some delta up or down."
      ],
      "metadata": {
        "id": "ZEt2a7DD6jss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff = nn.Linear(d_embed, d_embed)\n",
        "x = ff(x)\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "_EGA4t_myfIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.5 Add residual back in"
      ],
      "metadata": {
        "id": "BX-aPZGO7-bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x_residual + x\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "1ECgQ-hr7_QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Feed Forward Module"
      ],
      "metadata": {
        "id": "MwVW4AGcA_jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a straight-forward decoding and re-encoding of the embedding plus self-attention. What we want by the end of the encoding stage is a hidden state. Like in a sequence-to-sequence network we want a *stack* of hidden states, one for each token. That way, the decoder will be able to look back and attend to the hidden state that will be most useful for decoding by looking just at this stack instead of iterating over all the input tokens. So whatever is in each token position has to be representative of what is going on in the input. To move the matrix toward a hidden state we expand the embeddings, giving the network some capacity, and then collapse it down again to force it to make trade-offs."
      ],
      "metadata": {
        "id": "xUg-4uJW7JH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.1 Set aside residual"
      ],
      "metadata": {
        "id": "_9B8fYjZ8PMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_residual = x.clone()\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "2msqv_Br6fHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.2 Pre-Feed-Forward Layer Normalization"
      ],
      "metadata": {
        "id": "APWv9beO1rki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = x.mean(-1, keepdim=True)\n",
        "std = x.std(-1, keepdim=True)\n",
        "W2 = nn.Parameter(torch.ones(d_embed))\n",
        "b2 = nn.Parameter(torch.zeros(d_embed))\n",
        "x = W2 * (x - mean) / (std + epsilon) + b2\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "i2riRAUL8RFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.3 Feed Forward"
      ],
      "metadata": {
        "id": "AwE944o96SDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feed forward module grows the embeddings and then compresses it again. This is part of process of transforming the outputs of the self-attention module into a hidden state encoding."
      ],
      "metadata": {
        "id": "zIwN6aUQk2dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_expand = nn.Linear(d_embed, d_ff)\n",
        "linear_compress = nn.Linear(d_ff, d_embed)\n",
        "x = linear_compress(F.relu(linear_expand(x)))\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "7aNAnARK6CRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.4 Add residual back in"
      ],
      "metadata": {
        "id": "3ksl3ra36qqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x_residual + x\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "VhWKoJsi6sec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Final Encoder Layer Normalization"
      ],
      "metadata": {
        "id": "qbm-9J7V8z7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After repeating the self-attention and feed forward sub-layers N times, we apply one last layer normalization."
      ],
      "metadata": {
        "id": "vrl8f-TEBxMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = x.mean(-1, keepdim=True)\n",
        "std = x.std(-1, keepdim=True)\n",
        "Wn = nn.Parameter(torch.ones(d_embed))\n",
        "bn = nn.Parameter(torch.zeros(d_embed))\n",
        "x = Wn * (x - mean) / (std + epsilon) + bn\n",
        "print(x.size())"
      ],
      "metadata": {
        "id": "Lz9Wf7cO6t8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we should have a matrix, stored in `x` that we can interpret as a stack of hidden states. The Decoder will attempt to attend to this stack and pick out (via softmax emulating argmax) the hidden state that is most helpful in guessing the work that goes in the masked position."
      ],
      "metadata": {
        "id": "V-Gbr1_UzQUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Signify that the output is the hidden state\n",
        "hidden = x\n",
        "print(hidden.size())"
      ],
      "metadata": {
        "id": "atpOCyod0Xkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Decoder"
      ],
      "metadata": {
        "id": "EBZ52DJo-Lry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decoder works a lot like the Encoder except for one major change. In addition to self-attention and a feed-forward modules, the Decoder will also include a *source-attention* module wherein it attends to the hidden state output of the encoder.\n",
        "\n",
        "We will be operating on `y`, which is the sequence of target tokens instead of `x`. It seems weird to be treating the target the same as an input. The closest analog is the sequence-to-sequence network, which would generate a sequence of output tokens one at a time to compare to the target sequence to compute loss. But here we don't need to generate the output sequence because there is no recurrence. So we just take the target output and treat it as if it was generated by the transformer. The exception is the masked output token (which is normally the same position as the masked input). For computing loss, we only care if we get a good prediction for the masked target tokens."
      ],
      "metadata": {
        "id": "7Y0CisCQzmtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Decoder Embeddings"
      ],
      "metadata": {
        "id": "gD3IQX2A-S9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_d = nn.Embedding(vocab, d_embed)\n",
        "y = emb_d(y) * math.sqrt(d_embed)\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "9gzaYeb8-NJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add positional embeddings."
      ],
      "metadata": {
        "id": "qIJG3QC2-VWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pe = torch.zeros(max_len, d_embed, requires_grad=False)\n",
        "position = torch.arange(0, max_len).unsqueeze(1)\n",
        "divisor = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
        "pe[:, 0::2] = torch.sin(position * divisor)\n",
        "pe[:, 1::2] = torch.cos(position * divisor)\n",
        "pe = pe.unsqueeze(0)\n",
        "y = y + pe[:, :y.size(1)]\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "gOXTa8yJ-VC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Decoder Attention Layers"
      ],
      "metadata": {
        "id": "YkI23w9Z_LzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder layers will be repeated N times. This code walkthrough will only take us through one. The Decoder Attention Layer consists of self-attention followed by a source-attention, followed by a feed forward. Each of these are wrapped with residuals."
      ],
      "metadata": {
        "id": "0UJrTA06DNoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Self-Attention Sub-Layer"
      ],
      "metadata": {
        "id": "nYKLH9tgEnVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.1 Set aside residual"
      ],
      "metadata": {
        "id": "0mgRzWgkAfFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_residual = y.clone()\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "0qZZBDBr_NRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.2 Pre-Self-Attention Layer Normalization"
      ],
      "metadata": {
        "id": "oX38ceiTAqp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = y.mean(-1, keepdim=True)\n",
        "std = y.std(-1, keepdim=True)\n",
        "W1_d = nn.Parameter(torch.ones(d_embed))\n",
        "b1_d = nn.Parameter(torch.zeros(d_embed))\n",
        "y = W1_d * (y - mean) / (std + epsilon) + b1_d\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "qfK0B9FEArrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.3 Self-Attention"
      ],
      "metadata": {
        "id": "1ZOJN-Y6AkgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = y\n",
        "q = y\n",
        "v = y\n",
        "linear_q_self = nn.Linear(d_embed, d_embed)\n",
        "linear_k_self = nn.Linear(d_embed, d_embed)\n",
        "linear_v_self = nn.Linear(d_embed, d_embed)\n",
        "d_k = d_embed // num_heads\n",
        "q = linear_q_self(q)\n",
        "k = linear_k_self(k)\n",
        "v = linear_v_self(v)\n",
        "q = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "k = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "v = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "print(\"q\", q.size())\n",
        "print(\"k\", k.size())\n",
        "print(\"v\", v.size())"
      ],
      "metadata": {
        "id": "OZF8LTbiBBNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.size(-1)\n",
        "scores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "scores = scores.masked_fill(y_mask == 0, -epsilon)\n",
        "attn = F.softmax(scores, dim = -1)\n",
        "print(\"attention\", attn.size())\n",
        "y = torch.matmul(attn, v)\n",
        "print(\"y\", y.size())"
      ],
      "metadata": {
        "id": "IWIeDIOpBJ10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assemble heads"
      ],
      "metadata": {
        "id": "6ato2UicBYqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.transpose(1, 2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "4BeJtRBjBX0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.4 Post-Self-Attention Feed Forward"
      ],
      "metadata": {
        "id": "2bierEXaBjR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff_d1 = nn.Linear(d_embed, d_embed)\n",
        "y = ff_d1(y)\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "FCnS-owhBioq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.5 Add residual back in"
      ],
      "metadata": {
        "id": "m1vjSeu7Bp7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y_residual + y\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "yLGImc73BpS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Source-Attention Sub-Layer"
      ],
      "metadata": {
        "id": "lmgiijtbFiXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.1 Set residual aside"
      ],
      "metadata": {
        "id": "dMk0234JBvJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_residual = y.clone()\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "afyYY7fmBzB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.2 Pre-Source-Attention Layer Normalization"
      ],
      "metadata": {
        "id": "2BxZbrnWB9aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = y.mean(-1, keepdim=True)\n",
        "std = y.std(-1, keepdim=True)\n",
        "W2_d = nn.Parameter(torch.ones(d_embed))\n",
        "b2_d = nn.Parameter(torch.zeros(d_embed))\n",
        "y = W2_d * (y - mean) / (std + epsilon) + b2_d\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "Z4fC-0REB8h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.3 Source Attention"
      ],
      "metadata": {
        "id": "88dSgLpqB4GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source attention works just like self-attention, except we compute the scores using keys and values from the encoder and apply it to the query from the decoder. That is, based on what the encoder thinks we should attend to, what part of the decoder sequence should we actually attend to."
      ],
      "metadata": {
        "id": "9gww-JtEFxx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = y\n",
        "k = hidden # notice we are using hidden\n",
        "v = hidden # notice we are using hidden\n",
        "linear_q_source = nn.Linear(d_embed, d_embed)\n",
        "linear_k_source = nn.Linear(d_embed, d_embed)\n",
        "linear_v_source = nn.Linear(d_embed, d_embed)\n",
        "d_k = d_embed // num_heads\n",
        "q = linear_q(q)\n",
        "k = linear_k(k)\n",
        "v = linear_v(v)\n",
        "q = q.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "k = k.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "v = v.view(num_batches, -1, num_heads, d_k).transpose(1, 2)\n",
        "print(\"q\", q.size())\n",
        "print(\"k\", k.size())\n",
        "print(\"v\", v.size())"
      ],
      "metadata": {
        "id": "JTz6K1Z-B3sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.size(-1)\n",
        "scores = torch.matmul(k, q.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "scores = scores.masked_fill(x_mask == 0, -epsilon) # note source mask\n",
        "attn = F.softmax(scores, dim = -1)\n",
        "print(\"attention\", attn.size())\n",
        "y = torch.matmul(attn, v)\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "HcRRIaAGC02k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assemble heads"
      ],
      "metadata": {
        "id": "cefcyfWDC4JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.transpose(1, 2).contiguous().view(num_batches, -1, num_heads * (d_embed // num_heads))\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "Rrj9aoLXC5Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.4 Post-Source-Attention Feed forward"
      ],
      "metadata": {
        "id": "3FfLZyREC832"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff_d2 = nn.Linear(d_embed, d_embed)\n",
        "y = ff_d2(y)\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "bUfjx2KhC9_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.5 Add residual back in"
      ],
      "metadata": {
        "id": "BcZiKusQDBjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y_residual + y\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "Dih1yg_kDDg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Feed Forward Sub-Layer"
      ],
      "metadata": {
        "id": "6FRfeSRBGbgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3.1 Set aside residual"
      ],
      "metadata": {
        "id": "Af-n0PTFDQ9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_residual = y.clone()\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "Bno9hwYxDSag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3.2 Pre-Feed-Forward Layer Normalization"
      ],
      "metadata": {
        "id": "xcz5yTDsDX8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = y.mean(-1, keepdim=True)\n",
        "std = y.std(-1, keepdim=True)\n",
        "W3_d = nn.Parameter(torch.ones(d_embed))\n",
        "b3_d = nn.Parameter(torch.zeros(d_embed))\n",
        "y = W3_d * (y - mean) / (std + epsilon) + b3_d\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "Ncx0PcG-DYt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3.3 Feed Forward"
      ],
      "metadata": {
        "id": "or2ZZdXfDegg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_expand_d = nn.Linear(d_embed, d_ff)\n",
        "linear_compress_d = nn.Linear(d_ff, d_embed)\n",
        "y = linear_compress_d(F.relu(linear_expand_d(y)))\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "YHrTcy-xDgaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3.4 Add residual back in"
      ],
      "metadata": {
        "id": "41h94ULPDn52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y_residual + y\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "iJ7CK78HDpQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Final Decoder Layer Normalization"
      ],
      "metadata": {
        "id": "lyxSbVuxDseS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = y.mean(-1, keepdim=True)\n",
        "std = y.std(-1, keepdim=True)\n",
        "Wn_d = nn.Parameter(torch.ones(d_embed))\n",
        "bn_d = nn.Parameter(torch.zeros(d_embed))\n",
        "y = Wn_d * (y - mean) / (std + epsilon) + bn_d\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "ld4nj8XJD0DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generate Probability Distribution"
      ],
      "metadata": {
        "id": "HMYkSaeIEL33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next module sits on top of the decoder and expands the decoder output into a log probability distribution over the vocabulary for each token position. This is done for all tokens, though the only ones that will matter for loss computation are the ones that are masked. The loss calculation is not done here."
      ],
      "metadata": {
        "id": "UDYHcYJiHZWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_scores = nn.Linear(d_embed, vocab)\n",
        "probs = F.log_softmax(linear_scores(y), dim=-1)\n",
        "print(probs.size())"
      ],
      "metadata": {
        "id": "BvdmMdREEOtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Training"
      ],
      "metadata": {
        "id": "OBOyeEY76IKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook does not go through loss computation and training at this time. Loss is computed by looking at the masked probabilities and measuring the KL divergence from the actual target tokens. The code above will work with at `.backward()` once, but it recreates the linear layers etc. from scratch each time so it won't really learn anything. See [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) for a version that is closer to implementation as well as more in-depth description of the loss computation and training loop."
      ],
      "metadata": {
        "id": "louGBtN_6MVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etc."
      ],
      "metadata": {
        "id": "2VD-nZuHyOJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Careful observers will notice that I have left out a few details, such as Dropout layers, which appear in various places in the actual implementation. These details improving learning but do not significantly alter the understanding of how Transformers work."
      ],
      "metadata": {
        "id": "-N_xmdHnyQkh"
      }
    }
  ]
}